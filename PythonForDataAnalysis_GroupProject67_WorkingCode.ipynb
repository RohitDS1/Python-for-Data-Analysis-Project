{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APANPS5210 - Python for Data Analysis\n",
    "## Group 67 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Pld8k7n4iB"
   },
   "source": [
    "# SETUP\n",
    "\n",
    "`pip install` is a command that you would run in the command prompt or terminal to install a package globally on your machine. However, in a Jupyter Notebook, you are running code in a Python environment that is specific to that notebook. Therefore, running `pip install` alone in a Jupyter Notebook cell will not install the package in the correct environment.\n",
    "\n",
    "On the other hand, `!pip install` is a Jupyter Notebook magic command that runs the pip command in the current notebook environment. It ensures that the package is installed in the correct environment associated with the notebook.\n",
    "\n",
    "Therefore, when you want to install a package in a Jupyter Notebook, you should use `!pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFGkncPmmo9I",
    "outputId": "68c8a639-114a-4fc9-a302-6636e6373e5e"
   },
   "outputs": [],
   "source": [
    "!pip install py_stringsimjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJk3VICCn3HB",
    "outputId": "dc1abde6-9da1-4cae-e60a-687e24f05b2f"
   },
   "outputs": [],
   "source": [
    "!pip install py_stringmatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc8nsaepn6qh",
    "outputId": "6f7cde96-f6d6-4b23-93e7-5c554b692fd4"
   },
   "outputs": [],
   "source": [
    "!pip install ssj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9c3k-t7oB73",
    "outputId": "4a1dcd8c-7e8b-4f98-cba1-871b7435c037"
   },
   "outputs": [],
   "source": [
    "# We didn't use this \n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0o_yYWeoF39"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import py_stringsimjoin as ssj\n",
    "import py_stringmatching as sm\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from py_stringsimjoin.join.jaccard_join import jaccard_join\n",
    "import ssj\n",
    "from fuzzywuzzy import process \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP69Wc8-uFe8"
   },
   "outputs": [],
   "source": [
    "# Local environment with VS Code\n",
    "\n",
    "left_raw = pd.read_csv(\"/Users/rohit/Documents/Rohit - Master's Applied Analytics/Term 2 Courses/Python for Data Analysis/Group Project/left_dataset.csv\")\n",
    "right_raw = pd.read_csv(\"/Users/rohit/Documents/Rohit - Master's Applied Analytics/Term 2 Courses/Python for Data Analysis/Group Project/right_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tApfgTmvtiXL"
   },
   "outputs": [],
   "source": [
    "# This way we keep the original raw data as a backup if we incorrectly modify it later\n",
    "left = left_raw\n",
    "right = right_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwACA3LBpLF8"
   },
   "source": [
    "# FUNCTIONS\n",
    "This should run in a separate .py file and we should call the functions like this:\n",
    "```\n",
    "from functions.py import missing_count, missing_drop\n",
    "from functions.py import fix_zip_codes\n",
    "from functions.py import lowercase\n",
    "from functions.py import remove_substring, remove_special_chars\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiDwMCL1pbDi"
   },
   "outputs": [],
   "source": [
    "# Counts the missing values in each column of df\n",
    "def missing_count(df):\n",
    "    result = df.isna().sum()\n",
    "    print(result)\n",
    "\n",
    "# Drops rows with missing values\n",
    "def missing_drop(df):\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "# Standardizes zip code\n",
    "def fix_zip_codes(text):\n",
    "    text = str(text)\n",
    "    text = text.split(\"-\")[0] if \"-\" in text else text\n",
    "    text = str(int(float(text))) if \".\" in text else text\n",
    "    return text\n",
    "\n",
    "    \n",
    "# Everything to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Remove substrings like LLC and INC\n",
    "def remove_substring(text, substring_list):  # como se hac√≠a para que default sea remove_substring(text, ['llc', 'inc']\n",
    "    for substring in substring_list:\n",
    "        text = text.replace(substring, '')\n",
    "    return text    \n",
    "\n",
    "\n",
    "# Remove special characters like , . / -\n",
    "def remove_special_chars(text, regex_pattern): \n",
    "    return re.sub(regex_pattern, '', text)\n",
    "# remove_special_chars(text, r'[^a-zA-Z0-9\\s]+'))\n",
    "\n",
    "\n",
    "# Remove numbers from the address # don't like it anymore\n",
    "def remove_numbers(address):\n",
    "    return re.sub(r'\\b\\d+(?![strndh])\\b', '', address) #keep numbers followed by ['st', 'nd', 'rd', 'th'] because indicates a street name\n",
    "# left['address_str'].apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qP1bvRswxwI"
   },
   "source": [
    "Our first appoach was the following.  Later we figured the second (final) approach was more flexible, as we can create a new column and not modify the original one\n",
    "\n",
    "```\n",
    "# FIX ZIP CODES\n",
    "def fix_zip_codes(df, column_name):\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.split(\"-\")[0] if \"-\" in x else x)\n",
    "    df[column_name] = df[column_name].apply(lambda x: str(int(float(x))) if \".\" in x else x)\n",
    "\n",
    "# calls like this:\n",
    "fix_zip_codes(left, 'postal_code')\n",
    "\n",
    "\n",
    "# LOWERCASE\n",
    "def lowercase(df, column_name):\n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "\n",
    "# calls like this: \n",
    "lowercase(left, 'name')\n",
    "\n",
    "\n",
    "# REMOVE SUBSTR\n",
    "def remove_substring(df, column_name, substring_list):\n",
    "    for substring in substring_list:\n",
    "        df[column_name] = df[column_name].str.replace(substring, '')\n",
    "\n",
    "# calls like this:\n",
    "remove_substring(left, 'name', ['llc', 'inc'])\n",
    "\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "def remove_special_chars(df, column_name, regex_pattern):\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(regex_pattern, '', x))\n",
    "\n",
    "# calls like this:\n",
    "remove_special_chars(left, 'name', r'[^a-zA-Z0-9\\s]+')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmAy7x8_qxun"
   },
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWk-MwWuuQpo"
   },
   "source": [
    "To avoid matching n:n I will create row numbers (I think it's a good, but not-so-common, good practice when working with SQL)\n",
    "\n",
    "The possible keys are:\n",
    "* naz: name, address, zip_code\n",
    "* na: name, address\n",
    "* nz: name, zip_code\n",
    "\n",
    "But first, we will clean the dataframes and call the functions to prepare our raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffda9EtXvrvD"
   },
   "source": [
    "### Make the columns of both dataframes the same, so it's easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eoAmPUA8vvd-",
    "outputId": "8d0ba8f6-b2bb-4152-ea2f-c809ea1291cc"
   },
   "outputs": [],
   "source": [
    "left.columns, right.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwPqByGmvwis",
    "outputId": "56dfba86-201a-4909-8638-6e700c12236b"
   },
   "outputs": [],
   "source": [
    "# Drop columns that don't \"match\" in the opposing dataset\n",
    "left = left.drop(columns = ['categories'])\n",
    "right = right.drop(columns = ['size'])\n",
    "\n",
    "left.shape, right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_NTDrtcv0CM",
    "outputId": "88490b9c-e797-4867-8a26-a29c45531498"
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "left = left.rename(columns = {'postal_code': 'zip_code'})\n",
    "left = left.rename(columns = {'entity_id': 'left_id'})\n",
    "right = right.rename(columns = {'business_id': 'right_id'})\n",
    "\n",
    "left.columns, right.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YwBX4N0wMqg",
    "outputId": "9dbbd84f-b160-45c1-ae5f-d326999273c2"
   },
   "outputs": [],
   "source": [
    "# This was important because some functions returned an error because of missing values\n",
    "\n",
    "print('left', missing_count(left))   \n",
    "print('right', missing_count(right)) # right doesnt' have missing values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvaKvf1UwZPU",
    "outputId": "f9af6b06-f1f8-4f3b-9c71-6b2ace828a2c"
   },
   "outputs": [],
   "source": [
    "right = missing_drop(right) \n",
    "right_raw.shape, right.shape  # right doesnt' have missing values (it doesn't print the result)\n",
    "\n",
    "left = missing_drop(left)\n",
    "left_raw.shape, left.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "293SX3VUwi04"
   },
   "source": [
    "## Now we move on to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUlq8pTTvF4q"
   },
   "outputs": [],
   "source": [
    "# FIX ZIP CODES\n",
    "\n",
    "left['zip_code_str'] = left['zip_code'].apply(fix_zip_codes)\n",
    "right['zip_code_str'] = right['zip_code'].apply(fix_zip_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWO5Q0EFwpqp"
   },
   "outputs": [],
   "source": [
    "# LOWERCASE\n",
    "\n",
    "left['name_str'] = left['name'].apply(lowercase)\n",
    "left['address_str'] = left['address'].apply(lowercase)\n",
    "left['city_str'] = left['city'].apply(lowercase)\n",
    "\n",
    "right['name_str'] = right['name'].apply(lowercase)\n",
    "right['address_str'] = right['address'].apply(lowercase)\n",
    "right['city_str'] = right['city'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuXcAipwxt5a"
   },
   "outputs": [],
   "source": [
    "# REMOVE SUBSTRING\n",
    "\n",
    "left['name_str'] = left['name_str'].apply(lambda x: remove_substring(x, ['llc', 'inc']))\n",
    "right['name_str'] = right['name_str'].apply(lambda x: remove_substring(x, ['llc', 'inc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iMaSU2tx3Ql"
   },
   "outputs": [],
   "source": [
    "# REMOVE SPECIAL CHARACTERS\n",
    "\n",
    "left['name_str'] = left['name_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "right['name_str'] = right['name_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "\n",
    "left['address_str'] = left['address_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "right['address_str'] = right['address_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "dNopH5DHzJqJ",
    "outputId": "d4b4ef13-f6da-4607-ffc2-dc44f36d4225"
   },
   "outputs": [],
   "source": [
    "left.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "R8AHNr47zN2A",
    "outputId": "3c18e786-1e1b-42e1-cf05-4ece65bc4e4b"
   },
   "outputs": [],
   "source": [
    "right.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsnkJFALyBE1"
   },
   "source": [
    "## ROW NUMBER\n",
    "\n",
    "To avoid matching n:n we will create row numbers (it's a good, but not-so-common, good practice when working with SQL)\n",
    "\n",
    "The possible keys are:\n",
    "* naz: name, address, zip_code\n",
    "* na: name, address\n",
    "* nz: name, zip_code\n",
    "\n",
    "This will be an aux to build our keys and avoid duplicate records when we join the datasets. The SQL-query-like we built this from was as follows:\n",
    "\n",
    "```\n",
    "SELECT\n",
    "\tname,\n",
    "\taddress,\n",
    "\tzip_code,\n",
    "\tROW_NUMBER() OVER(PARTITION BY name,address ORDER BY zip_code DESC) AS row_num\n",
    "FROM\n",
    "\tleft\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CHPd2Jo3YiG",
    "outputId": "5b2aec8a-4d43-4210-c8e5-263bc2608781"
   },
   "outputs": [],
   "source": [
    "# Note that there are no duplicate records\n",
    "\n",
    "print(f\"Total records in the left dataframe {len(left)}. Records after dropping duplicates {len(left.drop_duplicates())}\")\n",
    "print(f\"Total records in the left dataframe {len(right)}. Records after dropping duplicates {len(right.drop_duplicates())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkxH-7Nuytpi",
    "outputId": "6f97b386-c0a0-4d4d-ea86-914cbee1929b"
   },
   "outputs": [],
   "source": [
    "left['row_num_naz'] = (\n",
    "    left.sort_values(['name_str', 'address_str', 'zip_code_str', 'left_id'], ascending = [True, True, True, False])\n",
    "    .groupby(['name_str', 'address_str', 'zip_code_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are no duplicated records if we consider the key [name, address, zip_code]\n",
    "print(len(left[left['row_num_naz'] > 1]))\n",
    "\n",
    "# So we can drop the column we just created\n",
    "left = left.drop(columns = ['row_num_naz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSaNvej_3GyO",
    "outputId": "8865a8e8-ef00-48c0-8db6-23f37159885f"
   },
   "outputs": [],
   "source": [
    "right['row_num_naz'] = (\n",
    "    right.sort_values(['name_str', 'address_str', 'zip_code_str', 'right_id'], ascending = [True, True, True, False])\n",
    "    .groupby(['name_str', 'address_str', 'zip_code_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are no duplicate records if we consider the key [name, address, zip_code]\n",
    "print(len(right[right['row_num_naz'] > 1]))\n",
    "\n",
    "# So we can drop the column we just created\n",
    "right = right.drop(columns = ['row_num_naz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jmp3DpQH4u7J",
    "outputId": "2394d0a8-5972-46c7-9ef4-cde5306a343d"
   },
   "outputs": [],
   "source": [
    "left['row_num_na'] = (\n",
    "    left.sort_values(['name_str', 'address_str', 'zip_code_str'], ascending = [True, True, False])\n",
    "    .groupby(['name_str', 'address_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are 4 records that are duplicated if we consider the key [name, address]\n",
    "len(left[left['row_num_na'] > 1])\n",
    "\n",
    "print(f\"There are {len(left[left['row_num_na'] > 1])} records that are duplicated if we consider the key [name, address]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vq9cJExgPYjx",
    "outputId": "4f65db41-0f60-4b37-a106-93de0ef5886e"
   },
   "outputs": [],
   "source": [
    "right['row_num_na'] = (\n",
    "    right.sort_values(['name_str', 'address_str', 'zip_code_str'], ascending = [True, True, False])\n",
    "    .groupby(['name_str', 'address_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are 42 records that are duplicated if we consider the key [name, address]\n",
    "len(right[right['row_num_na'] > 1])\n",
    "\n",
    "print(f\"There are {len(right[right['row_num_na'] > 1])} records that are duplicated if we consider the key [name, address]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGHr9plCPvxi",
    "outputId": "d38d6d08-c608-4172-c6c0-e16824725273"
   },
   "outputs": [],
   "source": [
    "left['row_num_nz'] = (\n",
    "    left.sort_values(['name_str', 'zip_code_str', 'left_id'], ascending = [True, True, False])\n",
    "    .groupby(['name_str', 'zip_code_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are no duplicate records if we consider the key [name, zip_code]\n",
    "print(len(left[left['row_num_nz'] > 1]))\n",
    "\n",
    "# So we can drop the column we just created\n",
    "left = left.drop(columns = ['row_num_nz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXDMZomwPvM9",
    "outputId": "755c1530-ae61-42b7-8ef2-73dac4cd15ca"
   },
   "outputs": [],
   "source": [
    "right['row_num_nz'] = (\n",
    "    right.sort_values(['name_str', 'zip_code_str', 'right_id'], ascending = [True, True, False])\n",
    "    .groupby(['name_str', 'zip_code_str'])\n",
    "    .zip_code_str\n",
    "    .rank(method = 'dense')\n",
    "    )\n",
    "\n",
    "# There are no duplicate records if we consider the key [name, zip_code]\n",
    "print(len(right[right['row_num_nz'] > 1]))\n",
    "\n",
    "# So we can drop the column we just created\n",
    "right = right.drop(columns = ['row_num_nz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f1sNzidvMQV",
    "outputId": "8e56d4e8-a824-4f86-fcaa-8e4699a47783"
   },
   "outputs": [],
   "source": [
    "left['row_num_na'] = str(left['row_num_na'])\n",
    "right['row_num_na'] = str(right['row_num_na'])\n",
    "\n",
    "left.info(), right.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38bVr464pD1M"
   },
   "source": [
    "# DATA EXPLORATION / VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "W805QE3Aqncj",
    "outputId": "bb572edb-27fc-4eb6-acff-061258b567b6"
   },
   "outputs": [],
   "source": [
    "# Get a list of all states\n",
    "states = [state for state in left_raw['state'] if isinstance(state, str)]\n",
    "\n",
    "# Count the number of occurrences of each state\n",
    "state_counts = pd.value_counts(states)\n",
    "\n",
    "# Create lists of labels and values for the pie chart\n",
    "labels, values = zip(*state_counts.items())\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Distribution of States in Left Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "dVh6r-C7QRpJ",
    "outputId": "07e7ee01-c0f9-47d0-89e7-cc6e5605baac"
   },
   "outputs": [],
   "source": [
    "# Get a list of all states\n",
    "states = [state for state in right_raw['state'] if isinstance(state, str)]\n",
    "\n",
    "# Count the number of occurrences of each state\n",
    "state_counts = pd.value_counts(states)\n",
    "\n",
    "# Create lists of labels and values for the pie chart\n",
    "labels, values = zip(*state_counts.items())\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Distribution of States in Right Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "Bq917xv2QVOF",
    "outputId": "d57d5821-0a2e-42d1-95fe-9f1cd1472da4"
   },
   "outputs": [],
   "source": [
    "# Top 5 States with the Most Businesses in the left Dataset\n",
    "state_counts = left_raw['state'].value_counts().nlargest(5)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(state_counts.index, state_counts.values, color='purple')\n",
    "plt.title('Top 5 States with the Most Businesses (Left Dataset)', fontsize=16)\n",
    "plt.xlabel('State', fontsize=14)\n",
    "plt.ylabel('Number of Businesses', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "xdsXg0OkQd5r",
    "outputId": "3890816f-fca7-4bb6-8fcd-ddcb7c6710f0"
   },
   "outputs": [],
   "source": [
    "# Top 10 Cities with the Most Businesses in the Right Dataset\n",
    "city_counts = right_raw['city'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(city_counts.index, city_counts.values, color='orange')\n",
    "plt.title('Top 10 Cities with the Most Businesses (Right Dataset)', fontsize=16)\n",
    "plt.xlabel('City', fontsize=14)\n",
    "plt.ylabel('Number of Businesses', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "vrwSjVlpQiqY",
    "outputId": "82d26ddc-e69c-471c-c191-d40a6154881c"
   },
   "outputs": [],
   "source": [
    "# Top 5 Categories in the Left Dataset\n",
    "# Create a list of all categories\n",
    "all_categories = []\n",
    "for cats in left_raw['categories']:\n",
    "    if isinstance(cats, str):\n",
    "        all_categories.extend(cats.split(','))\n",
    "\n",
    "# Count the occurrence of each category\n",
    "category_counts = {}\n",
    "for category in all_categories:\n",
    "    category_counts[category.strip()] = category_counts.get(category.strip(), 0) + 1\n",
    "\n",
    "# Sort the categories by count and get the top 5\n",
    "top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Create lists of the top 5 categories and their counts\n",
    "labels = [x[0] for x in top_categories]\n",
    "values = [x[1] for x in top_categories]\n",
    "\n",
    "# Create a bar chart of the top 5 categories\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, values)\n",
    "plt.title('Top 5 Categories (Left Dataset)', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp3qjr_OpG1w"
   },
   "source": [
    "# SQL-like joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9wn4Xj_ZQuw",
    "outputId": "db8c2dc7-f018-45d9-d48a-98e14f8467af"
   },
   "outputs": [],
   "source": [
    "# drop \"useless\" columns so it does not get supper messy\n",
    "\n",
    "left_df = left.drop(columns = ['name', 'address', 'city', 'zip_code', 'state'])\n",
    "right_df = right.drop(columns = ['name', 'address', 'city', 'zip_code', 'state'])\n",
    "\n",
    "print(f\"nrows in left_df: {len(left_df)}\")  #, print(f\"nrows in left_raw: {len(left_raw)}\")\n",
    "print(f\"nrows in right_df: {len(right_df)}\")#, print(f\"nrows in right_raw: {len(right_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVnjdadKq2nn",
    "outputId": "51b101e2-1833-498d-e464-2801a0153764"
   },
   "outputs": [],
   "source": [
    "join_naz = pd.merge(left_df, right_df, how= 'outer', on= ['name_str', 'address_str', 'zip_code_str'], indicator=True) # this key had no duplicates, no row_number\n",
    "print(\"Key: ['name_str', 'address_str', 'zip_code_str']\")\n",
    "\n",
    "# no duplicated records after the join\n",
    "print(f\"no duplicated records: left {join_naz['left_id'].nunique()}, right {join_naz['right_id'].nunique()}\")\n",
    "\n",
    "# records that don't match\n",
    "print(f\"no match {len(join_naz[join_naz['left_id'].isnull() | join_naz['right_id'].isnull()])}\")\n",
    "print(f\"match {len(join_naz[join_naz['left_id'].notnull() & join_naz['right_id'].notnull()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hySbWEjDhXu_",
    "outputId": "c74a9f3a-245d-4655-c359-25f7bb00b5a7"
   },
   "outputs": [],
   "source": [
    "join_na = pd.merge(left_df, right_df, how= 'outer', on= ['name_str', 'address_str', 'row_num_na'], indicator=True)\n",
    "print(\"Key: ['name_str', 'address_str', 'row_num_na']\")\n",
    "\n",
    "# no duplicated records after the join\n",
    "print(f\"no duplicated records: left {join_na['left_id'].nunique()}, right {join_na['right_id'].nunique()}\")\n",
    "\n",
    "# records that don't match\n",
    "print(f\"no match {len(join_na[join_na['left_id'].isnull() | join_na['right_id'].isnull()])}\")\n",
    "print(f\"match {len(join_na[join_na['left_id'].notnull() & join_na['right_id'].notnull()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_gITuiKg9fr",
    "outputId": "9bb386dd-ea6f-473b-98a2-390bf87f982d"
   },
   "outputs": [],
   "source": [
    "join_nz = pd.merge(left_df, right_df, how= 'outer', on= ['name_str', 'zip_code_str'], indicator=True) # this key had no duplicates, no row_number\n",
    "print(\"Key: ['name_str', 'zip_code_str']\")\n",
    "\n",
    "# no duplicated records after the join\n",
    "print(f\"no duplicated records: left {join_nz['left_id'].nunique()}, right {join_nz['right_id'].nunique()}\")\n",
    "\n",
    "# records that don't match\n",
    "print(f\"no match {len(join_nz[join_nz['left_id'].isnull() | join_nz['right_id'].isnull()])}\")\n",
    "print(f\"match {len(join_nz[join_nz['left_id'].notnull() & join_nz['right_id'].notnull()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTYR3VfyisF7"
   },
   "source": [
    "All these joins were just an excercise.  Remember that *the goal of the project is to find businesses that have a name and address that match between the left and right datasets.* <br>\n",
    "**It is interesting to note that we have an exact match only on 569 records, which is about 0.9% of each dataset.** <br>\n",
    "We will move on and use a matching algorithm using as a key the name, address and row_number.  The latter is important because, since we did some transformations to the name and address, it is useful to incorporate the row_number to avoid matching one row of the left dataset to mutiple rows on the right dataset, and viceversa.  That would overestimate the number of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwhPI3hl6HmQ"
   },
   "source": [
    "# MATCHING ALGORITHMS - JACCARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOu7Si-ERpkP"
   },
   "source": [
    "## ROHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuGs9KaLRsDu",
    "outputId": "bba7fbdc-aeb7-4e8b-9e1a-e05040caa00a"
   },
   "outputs": [],
   "source": [
    "A = left_raw\n",
    "B = right_raw\n",
    "\n",
    "print('Number of records in A: ' + str(len(A)))\n",
    "print('Number of records in B: ' + str(len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO8SyaRKR3rY",
    "outputId": "4ab8a6ad-3733-4f11-bd63-0edd2d91fa17"
   },
   "outputs": [],
   "source": [
    "#A.entity_id, you are selecting the column with the label 'entity_id' from the DataFrame A.\n",
    "A.entity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_ygkGuCPR8kK",
    "outputId": "44553f02-3d19-4d21-9040-125bbc5506e8"
   },
   "outputs": [],
   "source": [
    "# Create a new column in DataFrame B called 'new_key_attr', and assign a range of integers\n",
    "# from 0 to the length of B to this column. This creates a unique identifier for each row in B\n",
    "# which can be used for matching with the corresponding rows in DataFrame A during a fuzzy join operation.\n",
    "\n",
    "B['new_key_attr'] = range(0, len(B))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeKLeb8JR_FI"
   },
   "outputs": [],
   "source": [
    "#Define a function to capitalize the first letter of each word in the name column\n",
    "def capitalize_name(name):\n",
    "    name = name.str.lower().str.title()\n",
    "    return name\n",
    "\n",
    "#Updating the returned result into the right dataset - name column\n",
    "B.loc[:, 'name'] = capitalize_name(B.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-tSX0X-gSBf9",
    "outputId": "90f2d04a-a27c-4f34-defe-67ea8781cf8d"
   },
   "outputs": [],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvyaZNF9SDUO"
   },
   "outputs": [],
   "source": [
    "# create whitespace tokenizer for tokenizing 'name' attribute. The return_set flag should be set to True since\n",
    "# Jaccard is a set based measure.\n",
    "ws = sm.WhitespaceTokenizer(return_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCo_x79DSFbx",
    "outputId": "7d7c3cd8-5051-4da1-ffbf-8bd4d7201f01"
   },
   "outputs": [],
   "source": [
    "# Use the ssj library to perform a fuzzy join between DataFrames A and B based on the 'name' attribute. The 'entity_id'\n",
    "# column from A will be matched against the 'business_id' column from B. The whitespace tokenizer object 'ws' will\n",
    "# be used to tokenize the 'name' attributes for both DataFrames. The Jaccard similarity threshold is set to 0.8,\n",
    "# meaning that pairs of rows with a Jaccard similarity score greater than or equal to 0.8 will be considered matches.\n",
    "# The output_pairs DataFrame will include the 'name' attribute for both A and B for all matched pairs.\n",
    "output_pairs_name = jaccard_join(A, B, 'entity_id', 'business_id', 'name', 'name', ws, 0.8,\n",
    "                                l_out_attrs=['name'], r_out_attrs=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "K06jKcH2SHee",
    "outputId": "71f25af8-789a-4634-dbe2-3aeecaf788e9"
   },
   "outputs": [],
   "source": [
    "output_pairs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXBAOLMuSKyS"
   },
   "outputs": [],
   "source": [
    "# Drop the '_id', 'l_name', and 'r_name' columns from the output_pairs DataFrame using the drop() method with the 'axis=1'\n",
    "# parameter to indicate that the columns should be dropped. The 'inplace=True' parameter ensures that the DataFrame is\n",
    "# modified in place rather than creating a new copy.\n",
    "\n",
    "output_pairs_name.drop(['_id', 'l_name', 'r_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NA23Mww3SNrL",
    "outputId": "0e6b0a2d-3015-49cc-da1a-52ecb59f88ae"
   },
   "outputs": [],
   "source": [
    "output_pairs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9SuRe-JSQvn",
    "outputId": "2b4508eb-4fc3-4984-dd3f-a1e9460103c5"
   },
   "outputs": [],
   "source": [
    "# Use the ssj library to perform a fuzzy join between DataFrames A and B based on the 'address' attribute. The 'entity_id'\n",
    "# column from A will be matched against the 'business_id' column from B. The whitespace tokenizer object 'ws' will\n",
    "# be used to tokenize the 'address' attributes for both DataFrames. The Jaccard similarity threshold is set to 0.8,\n",
    "# meaning that pairs of rows with a Jaccard similarity score greater than or equal to 0.8 will be considered matches.\n",
    "# The output_pairs_add DataFrame will include the 'address' attribute for both A and B for all matched pairs.\n",
    "\n",
    "\n",
    "output_pairs_address = jaccard_join(A, B, 'entity_id', 'business_id', 'address', 'address', ws, 0.8,\n",
    "                                l_out_attrs=['address'], r_out_attrs=['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8C0m6Q5STTe"
   },
   "outputs": [],
   "source": [
    "# Remove the '_id', 'l_address', and 'r_address' columns from the output_pairs_add DataFrame since they are not needed.\n",
    "# The 'axis=1' parameter specifies that the columns should be dropped, and the 'inplace=True' parameter specifies\n",
    "# that the changes should be made to the DataFrame in place, without creating a new copy.\n",
    "output_pairs_address.drop(['_id', 'l_address', 'r_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nBt43GLTSVLT",
    "outputId": "98aec694-979e-430f-e63c-e5852da1d7f4"
   },
   "outputs": [],
   "source": [
    "output_pairs_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsN0llwRSdL7"
   },
   "source": [
    "Generating CSV File combining the matched records for Address and Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUPNBnSpSXNa",
    "outputId": "3ce3b342-1837-4148-c340-902d74391f25"
   },
   "outputs": [],
   "source": [
    "# concatenate the 2 data frames\n",
    "final_result_df = pd.concat([output_pairs_name, output_pairs_address])\n",
    "\n",
    "\n",
    "#Total number of matched records for both address and name\n",
    "print(final_result_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPi7poNzSfcd"
   },
   "outputs": [],
   "source": [
    "# write the result data frame to a CSV file\n",
    "final_result_df.to_csv('FinalResult_MatchingRecords.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5CpIxg7ylSV"
   },
   "source": [
    "## FUZZY WUZZY\n",
    "\n",
    "## Another approach for matching records\n",
    "\n",
    "#### This approach takes a lot of time and hence we didn't go ahead with it. Also the code didn't completely execute and we had to interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkYnFcwm8voj",
    "outputId": "1f570f9b-4066-44d5-c17d-94069dbfcb4e"
   },
   "outputs": [],
   "source": [
    "'''left_df.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaNjH9oH83dx",
    "outputId": "037ebbdf-286f-4972-a25a-7961d954705d"
   },
   "outputs": [],
   "source": [
    "'''left_df['city_str'].value_counts(ascending = False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy Wuzzy Match Algorithm\n",
    "\n",
    "'''\n",
    "# import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# convert non-string values in the address column to NaN values and fill with an empty string\n",
    "left_df['address'] = left_df['address'].astype(str).replace('nan', '').fillna('')\n",
    "right_df['address'] = right_df['address'].astype(str).replace('nan', '').fillna('')\n",
    "\n",
    "def find_matching_records(left_df, right_df):\n",
    "    # create an empty dictionary to store the matches\n",
    "    matches = {}\n",
    "\n",
    "    # loop through each record in the left dataset\n",
    "    for left_index, left_row in left_df.iterrows():\n",
    "        name = left_row['name']\n",
    "       \n",
    "        # filter the right dataset based on state and zip code\n",
    "        right_records = right_df[(right_df['state'] == left_row['state'])]\n",
    "\n",
    "        # find the best match in the right dataset based on name and address\n",
    "        best_match = process.extractOne(name, right_records.apply(lambda row: row['name'], axis=1))\n",
    "        \n",
    "        # if a match with similarity score above 80 was found, add it to the dictionary\n",
    "        if best_match is not None and best_match[1] > 80:\n",
    "            match_name, score, right_index = best_match\n",
    "            matches[left_index] = right_index\n",
    "            \n",
    "    return matches\n",
    "\n",
    "# find the matching records between the two datasets\n",
    "matches = find_matching_records(left_df, right_df)\n",
    "\n",
    "# print the number of matches found\n",
    "print(len(matches))\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
