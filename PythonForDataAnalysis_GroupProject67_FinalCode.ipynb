{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APANPS5210 - Python for Data Analysis\n",
    "## Group 67 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Pld8k7n4iB"
   },
   "source": [
    "# SETUP\n",
    "\n",
    "`pip install` is a command that you would run in the command prompt or terminal to install a package globally on your machine. However, in a Jupyter Notebook, you are running code in a Python environment that is specific to that notebook. Therefore, running `pip install` alone in a Jupyter Notebook cell will not install the package in the correct environment.\n",
    "\n",
    "On the other hand, `!pip install` is a Jupyter Notebook magic command that runs the pip command in the current notebook environment. It ensures that the package is installed in the correct environment associated with the notebook.\n",
    "\n",
    "Therefore, when you want to install a package in a Jupyter Notebook, you should use `!pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFGkncPmmo9I",
    "outputId": "68c8a639-114a-4fc9-a302-6636e6373e5e"
   },
   "outputs": [],
   "source": [
    "!pip install py_stringsimjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJk3VICCn3HB",
    "outputId": "dc1abde6-9da1-4cae-e60a-687e24f05b2f"
   },
   "outputs": [],
   "source": [
    "!pip install py_stringmatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc8nsaepn6qh",
    "outputId": "6f7cde96-f6d6-4b23-93e7-5c554b692fd4"
   },
   "outputs": [],
   "source": [
    "!pip install ssj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9c3k-t7oB73",
    "outputId": "4a1dcd8c-7e8b-4f98-cba1-871b7435c037"
   },
   "outputs": [],
   "source": [
    "# We didn't use this \n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0o_yYWeoF39"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import py_stringsimjoin as ssj\n",
    "import py_stringmatching as sm\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from py_stringsimjoin.join.jaccard_join import jaccard_join\n",
    "import ssj\n",
    "from fuzzywuzzy import process \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP69Wc8-uFe8"
   },
   "outputs": [],
   "source": [
    "# Local environment with VS Code\n",
    "\n",
    "left_raw = pd.read_csv(\"/Users/rohit/Documents/Rohit - Master's Applied Analytics/Term 2 Courses/Python for Data Analysis/Group Project/left_dataset.csv\")\n",
    "right_raw = pd.read_csv(\"/Users/rohit/Documents/Rohit - Master's Applied Analytics/Term 2 Courses/Python for Data Analysis/Group Project/right_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tApfgTmvtiXL"
   },
   "outputs": [],
   "source": [
    "# This way we keep the original raw data as a backup if we incorrectly modify it later\n",
    "left = left_raw\n",
    "right = right_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwACA3LBpLF8"
   },
   "source": [
    "# FUNCTIONS\n",
    "This should run in a separate .py file and we should call the functions like this:\n",
    "```\n",
    "from functions.py import missing_count, missing_drop\n",
    "from functions.py import fix_zip_codes\n",
    "from functions.py import lowercase\n",
    "from functions.py import remove_substring, remove_special_chars\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiDwMCL1pbDi"
   },
   "outputs": [],
   "source": [
    "# Counts the missing values in each column of df\n",
    "def missing_count(df):\n",
    "    result = df.isna().sum()\n",
    "    print(result)\n",
    "\n",
    "# Drops rows with missing values\n",
    "def missing_drop(df):\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "# Standardizes zip code\n",
    "def fix_zip_codes(text):\n",
    "    text = str(text)\n",
    "    text = text.split(\"-\")[0] if \"-\" in text else text\n",
    "    text = str(int(float(text))) if \".\" in text else text\n",
    "    return text\n",
    "\n",
    "    \n",
    "# Everything to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Remove substrings like LLC and INC\n",
    "def remove_substring(text, substring_list):  # como se hac√≠a para que default sea remove_substring(text, ['llc', 'inc']\n",
    "    for substring in substring_list:\n",
    "        text = text.replace(substring, '')\n",
    "    return text    \n",
    "\n",
    "\n",
    "# Remove special characters like , . / -\n",
    "def remove_special_chars(text, regex_pattern): \n",
    "    return re.sub(regex_pattern, '', text)\n",
    "# remove_special_chars(text, r'[^a-zA-Z0-9\\s]+'))\n",
    "\n",
    "\n",
    "# Remove numbers from the address # don't like it anymore\n",
    "def remove_numbers(address):\n",
    "    return re.sub(r'\\b\\d+(?![strndh])\\b', '', address) #keep numbers followed by ['st', 'nd', 'rd', 'th'] because indicates a street name\n",
    "# left['address_str'].apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qP1bvRswxwI"
   },
   "source": [
    "Our first appoach was the following.  Later we figured the second (final) approach was more flexible, as we can create a new column and not modify the original one\n",
    "\n",
    "```\n",
    "# FIX ZIP CODES\n",
    "def fix_zip_codes(df, column_name):\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.split(\"-\")[0] if \"-\" in x else x)\n",
    "    df[column_name] = df[column_name].apply(lambda x: str(int(float(x))) if \".\" in x else x)\n",
    "\n",
    "# calls like this:\n",
    "fix_zip_codes(left, 'postal_code')\n",
    "\n",
    "\n",
    "# LOWERCASE\n",
    "def lowercase(df, column_name):\n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "\n",
    "# calls like this: \n",
    "lowercase(left, 'name')\n",
    "\n",
    "\n",
    "# REMOVE SUBSTR\n",
    "def remove_substring(df, column_name, substring_list):\n",
    "    for substring in substring_list:\n",
    "        df[column_name] = df[column_name].str.replace(substring, '')\n",
    "\n",
    "# calls like this:\n",
    "remove_substring(left, 'name', ['llc', 'inc'])\n",
    "\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "def remove_special_chars(df, column_name, regex_pattern):\n",
    "    df[column_name] = df[column_name].apply(lambda x: re.sub(regex_pattern, '', x))\n",
    "\n",
    "# calls like this:\n",
    "remove_special_chars(left, 'name', r'[^a-zA-Z0-9\\s]+')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmAy7x8_qxun"
   },
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWk-MwWuuQpo"
   },
   "source": [
    "To avoid matching n:n I will create row numbers (I think it's a good, but not-so-common, good practice when working with SQL)\n",
    "\n",
    "The possible keys are:\n",
    "* naz: name, address, zip_code\n",
    "* na: name, address\n",
    "* nz: name, zip_code\n",
    "\n",
    "But first, we will clean the dataframes and call the functions to prepare our raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffda9EtXvrvD"
   },
   "source": [
    "### Make the columns of both dataframes the same, so it's easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eoAmPUA8vvd-",
    "outputId": "8d0ba8f6-b2bb-4152-ea2f-c809ea1291cc"
   },
   "outputs": [],
   "source": [
    "left.columns, right.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwPqByGmvwis",
    "outputId": "56dfba86-201a-4909-8638-6e700c12236b"
   },
   "outputs": [],
   "source": [
    "# Drop columns that don't \"match\" in the opposing dataset\n",
    "left = left.drop(columns = ['categories'])\n",
    "right = right.drop(columns = ['size'])\n",
    "\n",
    "left.shape, right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_NTDrtcv0CM",
    "outputId": "88490b9c-e797-4867-8a26-a29c45531498"
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "left = left.rename(columns = {'postal_code': 'zip_code'})\n",
    "left = left.rename(columns = {'entity_id': 'left_id'})\n",
    "right = right.rename(columns = {'business_id': 'right_id'})\n",
    "\n",
    "left.columns, right.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YwBX4N0wMqg",
    "outputId": "9dbbd84f-b160-45c1-ae5f-d326999273c2"
   },
   "outputs": [],
   "source": [
    "# This was important because some functions returned an error because of missing values\n",
    "\n",
    "print('left', missing_count(left))   \n",
    "print('right', missing_count(right)) # right doesnt' have missing values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvaKvf1UwZPU",
    "outputId": "f9af6b06-f1f8-4f3b-9c71-6b2ace828a2c"
   },
   "outputs": [],
   "source": [
    "right = missing_drop(right) \n",
    "right_raw.shape, right.shape  # right doesnt' have missing values (it doesn't print the result)\n",
    "\n",
    "left = missing_drop(left)\n",
    "left_raw.shape, left.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "293SX3VUwi04"
   },
   "source": [
    "## Now we move on to the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUlq8pTTvF4q"
   },
   "outputs": [],
   "source": [
    "# FIX ZIP CODES\n",
    "\n",
    "left['zip_code_str'] = left['zip_code'].apply(fix_zip_codes)\n",
    "right['zip_code_str'] = right['zip_code'].apply(fix_zip_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWO5Q0EFwpqp"
   },
   "outputs": [],
   "source": [
    "# LOWERCASE\n",
    "\n",
    "left['name_str'] = left['name'].apply(lowercase)\n",
    "left['address_str'] = left['address'].apply(lowercase)\n",
    "left['city_str'] = left['city'].apply(lowercase)\n",
    "\n",
    "right['name_str'] = right['name'].apply(lowercase)\n",
    "right['address_str'] = right['address'].apply(lowercase)\n",
    "right['city_str'] = right['city'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuXcAipwxt5a"
   },
   "outputs": [],
   "source": [
    "# REMOVE SUBSTRING\n",
    "\n",
    "left['name_str'] = left['name_str'].apply(lambda x: remove_substring(x, ['llc', 'inc']))\n",
    "right['name_str'] = right['name_str'].apply(lambda x: remove_substring(x, ['llc', 'inc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iMaSU2tx3Ql"
   },
   "outputs": [],
   "source": [
    "# REMOVE SPECIAL CHARACTERS\n",
    "\n",
    "left['name_str'] = left['name_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "right['name_str'] = right['name_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "\n",
    "left['address_str'] = left['address_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))\n",
    "right['address_str'] = right['address_str'].apply(lambda x: remove_special_chars(x, r'[^a-zA-Z0-9\\s]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "dNopH5DHzJqJ",
    "outputId": "d4b4ef13-f6da-4607-ffc2-dc44f36d4225"
   },
   "outputs": [],
   "source": [
    "left.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "R8AHNr47zN2A",
    "outputId": "3c18e786-1e1b-42e1-cf05-4ece65bc4e4b"
   },
   "outputs": [],
   "source": [
    "right.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38bVr464pD1M"
   },
   "source": [
    "# DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "W805QE3Aqncj",
    "outputId": "bb572edb-27fc-4eb6-acff-061258b567b6"
   },
   "outputs": [],
   "source": [
    "# Get a list of all states\n",
    "states = [state for state in left_raw['state'] if isinstance(state, str)]\n",
    "\n",
    "# Count the number of occurrences of each state\n",
    "state_counts = pd.value_counts(states)\n",
    "\n",
    "# Create lists of labels and values for the pie chart\n",
    "labels, values = zip(*state_counts.items())\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Distribution of States in Left Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "dVh6r-C7QRpJ",
    "outputId": "07e7ee01-c0f9-47d0-89e7-cc6e5605baac"
   },
   "outputs": [],
   "source": [
    "# Get a list of all states\n",
    "states = [state for state in right_raw['state'] if isinstance(state, str)]\n",
    "\n",
    "# Count the number of occurrences of each state\n",
    "state_counts = pd.value_counts(states)\n",
    "\n",
    "# Create lists of labels and values for the pie chart\n",
    "labels, values = zip(*state_counts.items())\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Distribution of States in Right Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "Bq917xv2QVOF",
    "outputId": "d57d5821-0a2e-42d1-95fe-9f1cd1472da4"
   },
   "outputs": [],
   "source": [
    "# Top 5 States with the Most Businesses in the left Dataset\n",
    "state_counts = left_raw['state'].value_counts().nlargest(5)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(state_counts.index, state_counts.values, color='purple')\n",
    "plt.title('Top 5 States with the Most Businesses (Left Dataset)', fontsize=16)\n",
    "plt.xlabel('State', fontsize=14)\n",
    "plt.ylabel('Number of Businesses', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "xdsXg0OkQd5r",
    "outputId": "3890816f-fca7-4bb6-8fcd-ddcb7c6710f0"
   },
   "outputs": [],
   "source": [
    "# Top 10 Cities with the Most Businesses in the Right Dataset\n",
    "city_counts = right_raw['city'].value_counts().nlargest(10)\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(city_counts.index, city_counts.values, color='orange')\n",
    "plt.title('Top 10 Cities with the Most Businesses (Right Dataset)', fontsize=16)\n",
    "plt.xlabel('City', fontsize=14)\n",
    "plt.ylabel('Number of Businesses', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "vrwSjVlpQiqY",
    "outputId": "82d26ddc-e69c-471c-c191-d40a6154881c"
   },
   "outputs": [],
   "source": [
    "# Top 5 Categories in the Left Dataset\n",
    "# Create a list of all categories\n",
    "all_categories = []\n",
    "for cats in left_raw['categories']:\n",
    "    if isinstance(cats, str):\n",
    "        all_categories.extend(cats.split(','))\n",
    "\n",
    "# Count the occurrence of each category\n",
    "category_counts = {}\n",
    "for category in all_categories:\n",
    "    category_counts[category.strip()] = category_counts.get(category.strip(), 0) + 1\n",
    "\n",
    "# Sort the categories by count and get the top 5\n",
    "top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Create lists of the top 5 categories and their counts\n",
    "labels = [x[0] for x in top_categories]\n",
    "values = [x[1] for x in top_categories]\n",
    "\n",
    "# Create a bar chart of the top 5 categories\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, values)\n",
    "plt.title('Top 5 Categories (Left Dataset)', fontsize=16)\n",
    "plt.xlabel('Category', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwhPI3hl6HmQ"
   },
   "source": [
    "# MATCHING ALGORITHM - JACCARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuGs9KaLRsDu",
    "outputId": "bba7fbdc-aeb7-4e8b-9e1a-e05040caa00a"
   },
   "outputs": [],
   "source": [
    "A = left_raw\n",
    "B = right_raw\n",
    "\n",
    "print('Number of records in A: ' + str(len(A)))\n",
    "print('Number of records in B: ' + str(len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO8SyaRKR3rY",
    "outputId": "4ab8a6ad-3733-4f11-bd63-0edd2d91fa17"
   },
   "outputs": [],
   "source": [
    "#A.entity_id, you are selecting the column with the label 'entity_id' from the DataFrame A.\n",
    "A.entity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_ygkGuCPR8kK",
    "outputId": "44553f02-3d19-4d21-9040-125bbc5506e8"
   },
   "outputs": [],
   "source": [
    "# Create a new column in DataFrame B called 'new_key_attr', and assign a range of integers\n",
    "# from 0 to the length of B to this column. This creates a unique identifier for each row in B\n",
    "# which can be used for matching with the corresponding rows in DataFrame A during a fuzzy join operation.\n",
    "\n",
    "B['new_key_attr'] = range(0, len(B))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeKLeb8JR_FI"
   },
   "outputs": [],
   "source": [
    "#Define a function to capitalize the first letter of each word in the name column\n",
    "def capitalize_name(name):\n",
    "    name = name.str.lower().str.title()\n",
    "    return name\n",
    "\n",
    "#Updating the returned result into the right dataset - name column\n",
    "B.loc[:, 'name'] = capitalize_name(B.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-tSX0X-gSBf9",
    "outputId": "90f2d04a-a27c-4f34-defe-67ea8781cf8d"
   },
   "outputs": [],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvyaZNF9SDUO"
   },
   "outputs": [],
   "source": [
    "# create whitespace tokenizer for tokenizing 'name' attribute. The return_set flag should be set to True since\n",
    "# Jaccard is a set based measure.\n",
    "ws = sm.WhitespaceTokenizer(return_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCo_x79DSFbx",
    "outputId": "7d7c3cd8-5051-4da1-ffbf-8bd4d7201f01"
   },
   "outputs": [],
   "source": [
    "# Use the ssj library to perform a fuzzy join between DataFrames A and B based on the 'name' attribute. The 'entity_id'\n",
    "# column from A will be matched against the 'business_id' column from B. The whitespace tokenizer object 'ws' will\n",
    "# be used to tokenize the 'name' attributes for both DataFrames. The Jaccard similarity threshold is set to 0.8,\n",
    "# meaning that pairs of rows with a Jaccard similarity score greater than or equal to 0.8 will be considered matches.\n",
    "# The output_pairs DataFrame will include the 'name' attribute for both A and B for all matched pairs.\n",
    "output_pairs_name = jaccard_join(A, B, 'entity_id', 'business_id', 'name', 'name', ws, 0.8,\n",
    "                                l_out_attrs=['name'], r_out_attrs=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "K06jKcH2SHee",
    "outputId": "71f25af8-789a-4634-dbe2-3aeecaf788e9"
   },
   "outputs": [],
   "source": [
    "output_pairs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXBAOLMuSKyS"
   },
   "outputs": [],
   "source": [
    "# Drop the '_id', 'l_name', and 'r_name' columns from the output_pairs DataFrame using the drop() method with the 'axis=1'\n",
    "# parameter to indicate that the columns should be dropped. The 'inplace=True' parameter ensures that the DataFrame is\n",
    "# modified in place rather than creating a new copy.\n",
    "\n",
    "output_pairs_name.drop(['_id', 'l_name', 'r_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NA23Mww3SNrL",
    "outputId": "0e6b0a2d-3015-49cc-da1a-52ecb59f88ae"
   },
   "outputs": [],
   "source": [
    "output_pairs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9SuRe-JSQvn",
    "outputId": "2b4508eb-4fc3-4984-dd3f-a1e9460103c5"
   },
   "outputs": [],
   "source": [
    "# Use the ssj library to perform a fuzzy join between DataFrames A and B based on the 'address' attribute. The 'entity_id'\n",
    "# column from A will be matched against the 'business_id' column from B. The whitespace tokenizer object 'ws' will\n",
    "# be used to tokenize the 'address' attributes for both DataFrames. The Jaccard similarity threshold is set to 0.8,\n",
    "# meaning that pairs of rows with a Jaccard similarity score greater than or equal to 0.8 will be considered matches.\n",
    "# The output_pairs_add DataFrame will include the 'address' attribute for both A and B for all matched pairs.\n",
    "\n",
    "\n",
    "output_pairs_address = jaccard_join(A, B, 'entity_id', 'business_id', 'address', 'address', ws, 0.8,\n",
    "                                l_out_attrs=['address'], r_out_attrs=['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8C0m6Q5STTe"
   },
   "outputs": [],
   "source": [
    "# Remove the '_id', 'l_address', and 'r_address' columns from the output_pairs_add DataFrame since they are not needed.\n",
    "# The 'axis=1' parameter specifies that the columns should be dropped, and the 'inplace=True' parameter specifies\n",
    "# that the changes should be made to the DataFrame in place, without creating a new copy.\n",
    "output_pairs_address.drop(['_id', 'l_address', 'r_address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nBt43GLTSVLT",
    "outputId": "98aec694-979e-430f-e63c-e5852da1d7f4"
   },
   "outputs": [],
   "source": [
    "output_pairs_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsN0llwRSdL7"
   },
   "source": [
    "### Generating CSV File combining the matched records for Address and Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUPNBnSpSXNa",
    "outputId": "3ce3b342-1837-4148-c340-902d74391f25"
   },
   "outputs": [],
   "source": [
    "# concatenate the 2 data frames\n",
    "final_result_df = pd.concat([output_pairs_name, output_pairs_address])\n",
    "\n",
    "\n",
    "#Total number of matched records for both address and name\n",
    "print(final_result_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPi7poNzSfcd"
   },
   "outputs": [],
   "source": [
    "# write the result data frame to a CSV file\n",
    "final_result_df.to_csv('FinalResult_MatchingRecords.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
